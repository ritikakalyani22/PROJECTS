---
title: "R Notebook Group 11"
output:
  pdf_document: default
  html_notebook: default
---


## Loading all the required libraries:
```{r}
library(tidyverse)
library(ggplot2)
library(caret)
library(data.table)
library(dplyr)
library(glmnet)
library(corrplot)
library(doParallel)
library(randomForest)
library(nnet)
```
## Loading Given Datasets
```{r}
# Load required datasets
train_data <- read.csv('/Users/ritikakalyani/ADM Project/train_v3.csv')
test_data  <- read.csv('/Users/ritikakalyani/ADM Project/test_v3.csv')
test_no_loss <- read.csv('/Users/ritikakalyani/ADM Project/test__no_lossv3.csv')
```

#Data Exploration:

## Checking the dimensions of the dataset
```{r}
# Dimensions of the datasets
cat("Dimensions of the training dataset:", dim(train_data), "\n")
cat("Dimensions of the testing dataset:", dim(test_data), "\n")
cat("Dimensions of the test dataset with no loss column:", dim(test_no_loss), "\n")
```


## Checking structure of the training set
```{r}
# Overview of train data
str(train_data)
```
## There are total of 763 variables including the target variable and all of those are numeic.

## Checking the structure of testing dataset
```{r}
# Overview of test data
str(test_data)
```

## Printing first five rows of the training set to have a overview
```{r}
# Display specific columns directly
head(train_data[, c("f1", "f3", "f4","f777","loss")])

```

## Printing first five rows of the test set to have a overview
```{r}
head(test_data[, c("f1", "f3", "f4","f777","loss")])
```

## Summary of Train and Test dataset to understand the distribution
```{r}
summary(train_data[,1:10])
summary(train_data[,760:763])
```

```{r}
summary(test_data[,1:10])
summary(test_data[,760:763])
```
```{r}
summary(test_no_loss[,1:10])
summary(test_no_loss[,760:762])
```
# Data Visualization:

## Distribution of Loss column:
```{r}
library(ggplot2)

ggplot(train_data, aes(x = loss)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(
    title = "Distribution of Loss",
    x = "Loss",
    y = "Frequency"
  ) +
  theme_minimal()

```
## The distribution in the histogram is biased to the right. This indicates that the distribution decreases down towards greater loss values and that there are more data points with lower loss values.The mode, or the center of the distribution, seems to lie between 10 and 15.The values in the distribution range from about 0 to 80, making it fairly dispersed.


## Compaision of Loss values:
```{r}
library(dplyr)
library(ggplot2)

# Create the plot without permanently adding the 'loss_category' column
ggplot(train_data %>%
         mutate(loss_category = ifelse(loss == 0, "Zero", "Non-Zero")), 
       aes(x = loss_category)) +
  geom_bar(fill = c("skyblue", "coral"), alpha = 0.8) +
  labs(
    title = "Proportion of Zero vs Non-Zero Loss Values",
    x = "Loss Category",
    y = "Count"
  ) +
  theme_minimal()
``` 

## A larger number of observations in the dataset have a loss value of 0 compared to those with any non-zero loss values.


## Cumulative Percentage Line plot:
```{r}
# Create a cumulative summary
loss_cumulative <- train_data %>%
  group_by(loss) %>%
  summarize(count = n()) %>%
  mutate(cumulative_percentage = cumsum(count) / sum(count) * 100)

# Line plot of cumulative percentages
ggplot(loss_cumulative, aes(x = loss, y = cumulative_percentage)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "Cumulative Percentage of Loss Values",
    x = "Loss",
    y = "Cumulative Percentage"
  ) +
  theme_minimal()

```

## A considerable amount of the data has relatively low loss values, as indicated by the curve's sharp rise.After that, the curve flattens out, indicating that most of the data is inside a particular range of loss values.The curve then increases once more, although less rapidly, suggesting that fewer observations had larger loss values.



# Data Cleaning:

## Checking for total numberof  missing values in each dataset
```{r}
# Check for missing values in train,test and test_noloss datasets
cat("Total missing values in train data:", sum(is.na(train_data)), "\n")
#colSums(is.na(train_data))

cat("Total missing values in test data:", sum(is.na(test_data)), "\n")
#colSums(is.na(test_data))

cat("Total missing values in test data with no loss column:", sum(is.na(test_no_loss)), "\n")
#colSums(is.na(test_no_loss))
```


## Checking the missing range/percentage of the features in both training and testing sets

```{r}
# Calculate percentage of missing values
missing_percentage <- sapply(train_data, function(x) sum(is.na(x)) / nrow(train_data) * 100)

# Display columns with high missing percentages
high_missing <- data.frame(Column = names(missing_percentage), Missing_Percentage = missing_percentage)
high_missing <- high_missing %>% arrange(desc(Missing_Percentage))

# Find the minimum and maximum percentages
min_missing <- min(missing_percentage)
max_missing <- max(missing_percentage)

# Print the minimum and maximum percentages
cat("Minimum missing percentage:", min_missing, "%\n")
cat("Maximum missing percentage:", max_missing, "%\n")

# Drop columns with >50% missing values (if needed)
threshold <- 50
train_data <- train_data[, !(names(train_data) %in% high_missing$Column[high_missing$Missing_Percentage > threshold])]

```

```{r}
# Calculate percentage of missing values
missing_percentage_test <- sapply(test_data, function(x) sum(is.na(x)) / nrow(test_data) * 100)

# Display columns with high missing percentages
high_missing <- data.frame(Column = names(missing_percentage_test), Missing_Percentage = missing_percentage_test)
high_missing <- high_missing %>% arrange(desc(Missing_Percentage))

# Find the minimum and maximum percentages
min_missing <- min(missing_percentage_test)
max_missing <- max(missing_percentage_test)

# Print the minimum and maximum percentages
cat("Minimum missing percentage:", min_missing, "%\n")
cat("Maximum missing percentage:", max_missing, "%\n")

# Drop columns with >50% missing values (if needed)
threshold <- 50
test_data <- test_data[, !(names(test_data) %in% high_missing$Column[high_missing$Missing_Percentage > threshold])]
```


## Visualising the missing percentages:
```{r}
# Calculate percentage of missing values
missing_percentage_train <- sapply(train_data, function(x) sum(is.na(x)) / nrow(train_data) * 100)

# Create a data frame for easier plotting
missing_data_train <- data.frame(Column = names(missing_percentage_train), Missing_Percentage = missing_percentage_train)
# Find the maximum and minimum missing percentages
max_missing_train <- missing_data_train[which.max(missing_data_train$Missing_Percentage), ]
min_missing_train <- missing_data_train[which.min(missing_data_train$Missing_Percentage), ]

# Create a new data frame with only max and min columns
max_min_missing_train <- rbind(max_missing_train, min_missing_train)


# Create a bar plot for only the max and min missing percentages
ggplot(max_min_missing_train, aes(x = reorder(Column, Missing_Percentage), y = Missing_Percentage)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
   geom_text(aes(label = paste0(round(Missing_Percentage, 1), "%")), vjust = -0.5) + 
  theme_minimal() +
  coord_flip() +  # Flip the axis for better readability
  labs(title = "Max and Min Missing Data Percentages in Training data", 
       x = "Columns", 
       y = "Missing Percentage (%)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
# Calculate percentage of missing values for test data
missing_percentage_test <- sapply(test_data, function(x) sum(is.na(x)) / nrow(test_data) * 100)

# Create a data frame for easier plotting
missing_data_test <- data.frame(Column = names(missing_percentage_test), Missing_Percentage = missing_percentage_test)
# Find the maximum and minimum missing percentages in test data
max_missing_test <- missing_data_test[which.max(missing_data_test$Missing_Percentage), ]
min_missing_test <- missing_data_test[which.min(missing_data_test$Missing_Percentage), ]

# Create a new data frame with only max and min columns
max_min_missing_test <- rbind(max_missing_test, min_missing_test)

# Create a bar plot for max and min missing percentages in test data
ggplot(max_min_missing_test, aes(x = reorder(Column, Missing_Percentage), y = Missing_Percentage)) +
  geom_bar(stat = "identity", fill = "red", color = "black") +
  geom_text(aes(label = paste0(round(Missing_Percentage, 1), "%")), vjust = -0.5) +  # Add number labels
  theme_minimal() +
  coord_flip() +  # Flip the axis for better readability
  labs(title = "Max and Min Missing Data Percentages in Test Data", 
       x = "Columns", 
       y = "Missing Percentage (%)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

## Handling all the missing Values:

```{r}
# Step 1: Define a function to handle missing values for numeric data
handle_missing_numeric <- function(data) {
  # Replace missing values with the median of each column
  data <- data %>% 
    mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
  return(data)
}

# Step 2: Apply the function to train and test datasets
train_data <- handle_missing_numeric(train_data)
test_data <- handle_missing_numeric(test_data)
test_no_loss <- handle_missing_numeric(test_no_loss)

# Step 3: Verify that no missing values remain
cat("Remaining missing values in train data:", sum(is.na(train_data)), "\n")
cat("Remaining missing values in test data:", sum(is.na(test_data)), "\n")
cat("Remaining missing values in test_no_loss data:", sum(is.na(test_no_loss)), "\n")

```




```{r}
# Ensure loss variable is not removed
tail(colnames(train_data), 5)
tail(colnames(test_data), 5)
```


# Data Preprocessing:

## Before preprocessing the data making sure to remove the features with zero variance:

```{r}
# Removing zero variance features from datasets
# Step 1: Remove the target column before handling near-zero variance features
target <- train_data$loss  # Save the target variable
train_data <- train_data[, !colnames(train_data) %in% "loss"]  # Remove the target column

# Step 2: Identify near-zero variance features in the predictor features
nzv <- nearZeroVar(train_data, saveMetrics = TRUE)

# Step 3: Retain only features that are not near-zero variance
train_data <- train_data[, !nzv$nzv, drop = FALSE]

# Step 4: Add the target column back to the filtered train_data
train_data$loss <- target

# Step 5: Align test_data and test_no_loss with filtered train_data
test_data <- test_data[, colnames(test_data) %in% colnames(train_data), drop = FALSE]
test_no_loss <- test_no_loss[, colnames(test_no_loss) %in% colnames(train_data), drop = FALSE]

# Verify that all datasets now have consistent features
cat("Features in train_data (including target):", ncol(train_data), "\n")
cat("Features in test_data:", ncol(test_data), "\n")
cat("Features in test_no_loss:", ncol(test_no_loss), "\n")


```

## After removing the zero variance data there are 741 features remaining including the loss column that means there were 22 zero variance features which would not add any value to the prediction process.


## Now scaling the data:

```{r}
# Scaling the data
# Separate target variable (loss) from predictors
target <- train_data$loss  # Save the target variable
predictors <- train_data[, -which(names(train_data) == "loss")]  # Remove target column

# Apply standardization (center and scale) to predictor variables
preProc <- preProcess(predictors, method = c("center", "scale"))
predictors_scaled <- predict(preProc, predictors)

# Combine scaled predictors with the unscaled target
train_data_scaled <- cbind(predictors_scaled, loss = target)

# Similarly scale the test data (use the same preProc object)
test_data_scaled <- predict(preProc, test_data)
test_no_loss_scaled <- predict(preProc, test_no_loss)

```

```{r}
tail(colnames(train_data_scaled), 5)
tail(colnames(test_data_scaled), 5)
```


## Making sure all the columns are scaled:

```{r}
summary(train_data_scaled[,1:10])
```
## The mean is near to "0" means the data is scaled

```{r}
summary(test_data_scaled[,1:10])
```

```{r}
summary(test_no_loss_scaled[,1:10])
```

## Plotting train data to understand the outliers and distribution:

```{r}
boxplot(train_data_scaled[, 1:10], main = "Feature Distribution with Boxplots")
```
## The median is typically around the middle of the box, suggesting that the features have a fairly symmetric distribution because they are scaled.The dots outside the whiskers show that some features contain outliers.The data's variability or dispersion is indicated by the box sizes. The range of values for some attributes is greater than that of others.


```{r}
# Create boxplots for the first 10 features
boxplot(test_data_scaled[, 1:10], 
        main = "Feature Distribution with Boxplots", 
        col = "lightblue", 
        las = 2)

```
## The median is typically around the middle of the box, suggesting that the features have a fairly symmetric distribution because they are scaled.The dots outside the whiskers show that some features contain outliers.The data's variability or dispersion is indicated by the box sizes. The range of values for some attributes is greater than that of others.



# Feature Selection:

## Based on correlation:Highly correlated features are removed
```{r}
# Correlation-Based Feature Selection
# Step 1: Remove the target column (`loss`) from train_data for correlation filtering
target <- train_data$loss  # Save the target variable
train_data_no_target <- train_data[, !colnames(train_data) %in% "loss"]  # Remove `loss` column

# Step 2: Calculate correlation matrix and remove highly correlated features
correlation_matrix <- cor(train_data_no_target)
high_corr <- findCorrelation(correlation_matrix, cutoff = 0.9)  # Find correlated features
train_data_filtered <- train_data_no_target[, -high_corr]  # Remove highly correlated features

# Step 3: Add the `loss` column back to train_data_filtered
train_data_filtered$loss <- target

# Step 4: Align test_data while retaining its `loss` column
# Separate the `loss` column from test_data
test_data_loss <- test_data$loss  # Save the target variable from test_data
test_data_predictors <- test_data[, !colnames(test_data) %in% "loss"]  # Exclude `loss`

# Align test_data_predictors with filtered train_data_predictors
common_features_filtered <- colnames(train_data_filtered)[colnames(train_data_filtered) != "loss"]
test_data_filtered <- test_data_predictors[, common_features_filtered, drop = FALSE]

# Add the `loss` column back to test_data_filtered
test_data_filtered$loss <- test_data_loss

# Step 5: Align test_no_loss with filtered train_data predictors
test_no_loss_filtered <- test_no_loss[, common_features_filtered, drop = FALSE]

# Step 6: Verify alignment
cat("Features in train_data_filtered (including target):", ncol(train_data_filtered), "\n")
cat("Features in test_data_filtered (including target):", ncol(test_data_filtered), "\n")
cat("Features in test_no_loss_filtered:", ncol(test_no_loss_filtered), "\n")


```

## After removing highly correlated featues we are left with 274 features including the loss column.



## Checking feature alignment between datasets
```{r}
# Check feature alignment between datasets
identical(colnames(train_data_filtered)[-ncol(train_data_filtered)], colnames(test_data_filtered)[-ncol(test_data_filtered)])  # Should return TRUE
identical(colnames(train_data_filtered)[-ncol(train_data_filtered)], colnames(test_no_loss_filtered))  # Should return TRUE
```


## Using lasso regression for further feature selection:

```{r}
# Using Lasso Regression for Feature Selection
# Step 1: Prepare the data (ensure target variable 'loss' is separate)
X_train <- as.matrix(train_data_filtered[, -which(colnames(train_data_filtered) == "loss")])  # Predictor variables
y_train <- train_data_filtered$loss  # Target variable

# Step 2: Fit a Lasso model (alpha = 1 for Lasso)
lasso_model <- glmnet(X_train, y_train, alpha = 1)

# Step 3: Perform cross-validation to find the best lambda (regularization parameter)
fs_lasso <- cv.glmnet(X_train, y_train, alpha = 1)

```


## Printing the value of best lambda
```{r}
best_lambda <- fs_lasso$lambda.min  # Best lambda for minimum prediction error
cat("Best lambda:", best_lambda, "\n")
```


## Printing selected features by the lasso model
```{r}
# Step 4: Extract coefficients for the best lambda
lasso_coefficients <- coef(fs_lasso, s = best_lambda)

# Convert the coefficients object to a matrix
lasso_coefficients <- as.matrix(lasso_coefficients)

# Step 5: Identify non-zero coefficients (important features)
selected_features <- rownames(lasso_coefficients)[lasso_coefficients != 0]
selected_features <- selected_features[selected_features != "(Intercept)"]  # Exclude intercept
cat("Selected features:", length(selected_features), "\n")

# Step 6: Filter train_data, test_data, and test_no_loss based on selected features
train_data_lasso <- train_data_filtered[, c(selected_features, "loss"), drop = FALSE]
test_data_lasso <- test_data_filtered[, c(selected_features, "loss"), drop = FALSE]
test_no_loss_lasso <- test_no_loss_filtered[, selected_features, drop = FALSE]

```

## Lasso model has selected 69 important features out of 273 features.

```{r}
# Verify dimensions after feature selection
cat("Features in train_data_lasso (including target):", ncol(train_data_lasso), "\n")
cat("Features in test_data_lasso (including target):", ncol(test_data_lasso), "\n")
cat("Features in test_no_loss_lasso:", ncol(test_no_loss_lasso), "\n")
```

# Data Modeling:

## Now, splitting the data with lasso selected features for modeling

```{r}
# Splitting the data 
set.seed(123)  # For reproducibility
train_index <- createDataPartition(train_data_lasso$loss, p = 0.8, list = FALSE)

# Training and validation splits
train_split <- train_data_lasso[train_index, ]
valid_split <- train_data_lasso[-train_index, ]
```


## Building Lasso regression model:
```{r}
# Lasso regression model 
# Step 1: Prepare the data (exclude the target variable)
X_train <- as.matrix(train_split[, -which(colnames(train_split) == "loss")])  # Predictor variables
y_train <- train_split$loss  # Target variable

# Step 2: Fit Lasso model with cross-validation (cv.glmnet) to find optimal lambda
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)

# Best lambda from cross-validation
best_lambda <- cv_lasso$lambda.min
cat("Best lambda:", best_lambda, "\n")

# Step 3: Predict on the validation set using the best lambda
X_valid <- as.matrix(valid_split[, -which(colnames(valid_split) == "loss")])  # Predictor variables (no target)
y_valid <- valid_split$loss  # Target variable

# Make predictions using the optimal lambda
lasso_preds <- predict(cv_lasso, X_valid, s = "lambda.min")

# Evaluate performance using MAE (Mean Absolute Error)
lasso_mae <- mean(abs(y_valid - lasso_preds))
cat("Lasso MAE on validation set:", lasso_mae, "\n")



```


## Building Ridge regression model:
```{r}
library(glmnet)

# Step 1: Prepare the data
X_train <- as.matrix(train_split[, -which(colnames(train_split) == "loss")])  # Predictor variables
y_train <- train_split$loss  # Target variable

# Step 2: Fit Ridge regression model with cross-validation
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0)  # Alpha = 0 for Ridge
best_lambda <- cv_ridge$lambda.min  # Optimal lambda
cat("Best lambda for Ridge Regression:", best_lambda, "\n")

# Step 3: Prepare validation data
X_valid <- as.matrix(valid_split[, -which(colnames(valid_split) == "loss")])  # Validation predictors
y_valid <- valid_split$loss  # Validation target

# Step 4: Predict using the Ridge regression model
ridge_preds <- predict(cv_ridge, X_valid, s = best_lambda)

# Step 5: Evaluate performance using MAE
ridge_mae <- mean(abs(y_valid - ridge_preds))
cat("Ridge Regression MAE on validation set:", ridge_mae, "\n")

```


## Building Random forest model:
```{r}
# Enable parallel processing for faster processing
cl <- makeCluster(detectCores() - 2)
registerDoParallel(cl)

# Initialise cross-validation folds
train_control <- trainControl(method = "cv", number = 3, verboseIter = TRUE)

# Train a Random Forest model
rf_model <- train(loss ~ ., data = train_split, method = "rf",
                  trControl = train_control, 
                  tuneGrid = expand.grid(mtry = c(5, 10, 15)),
                  ntree = 200)  

# Stop parallel processing
stopCluster(cl)
registerDoSEQ()

# Predict on the validation set
rf_preds <- predict(rf_model, valid_split)

# Evaluate performance using Mean Absolute Error (MAE)
rf_mae <- mean(abs(valid_split$loss - rf_preds))
cat("Random Forest MAE on validation set:", rf_mae, "\n")

```


## Building the neural network model:
```{r}
# Neural network model
# Enable parallel processing for faster pocessing
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

numFolds <- trainControl(method = 'cv', number = 3, verboseIter = TRUE)

# Train a Neural Network model
nn_model <- train(loss ~ ., data = train_split, method = "nnet",
                  trControl = numFolds,tuneGrid = expand.grid(size = 3, decay = 0.1), 
                  maxit = 50)

# Stop parallel processing
stopCluster(cl)
registerDoSEQ()

print(nn_model)

# Predict on the validation set
nn_preds <- predict(nn_model, valid_split)

# Evaluate performance using Mean Absolute Error (MAE)
nn_mae <- mean(abs(valid_split$loss - nn_preds))
cat("Neural Network MAE on validation set:", nn_mae, "\n")

```


## Evaluating the models predictions on the given test set:

```{r}
# Predict usimg trained models on test dataset
# Prepare the test data for prediction
X_test <- as.matrix(test_data_lasso[, -which(colnames(test_data_lasso) == "loss")])

# Predict using the Lasso model
lasso_test_preds <- predict(cv_lasso, X_test, s = "lambda.min")

# Predict using the ridge model
ridge_test_preds <- predict(cv_ridge, X_test, s = "lambda.min")

# Predict using the Random Forest model
rf_test_preds <- predict(rf_model, test_data_lasso)

# Predict using the Neural Network model
nn_test_preds <- predict(nn_model, test_data_lasso)

# Calculate MAE for each model
lasso_mae <- mean(abs(test_data_lasso$loss - lasso_test_preds))
ridge_mae <- mean(abs(test_data_lasso$loss - ridge_test_preds))
rf_mae <- mean(abs(test_data_lasso$loss - rf_test_preds))
nn_mae <- mean(abs(test_data_lasso$loss - nn_test_preds))

# Print MAE results
cat("Lasso MAE on test set:", lasso_mae, "\n")
cat("Ridge MAE on test set:", ridge_mae, "\n")
cat("Random Forest MAE on test set:", rf_mae, "\n")
cat("Neural Network MAE on test set:", nn_mae, "\n")
```
```{r}
library(ggplot2)

# Step 1: Create a data frame with model names, MAE, and a highlight flag
model_performance <- data.frame(
  Model = c("Lasso", "Ridge", "Random Forest", "Neural Network"),
  MAE = c(lasso_mae, ridge_mae, rf_mae, nn_mae)
)

# Add a flag for the model with the lowest MAE
model_performance$Highlight <- ifelse(model_performance$MAE == min(model_performance$MAE), "Lowest MAE", "Other Models")

# Step 2: Create a bar plot with conditional coloring
ggplot(model_performance, aes(x = Model, y = MAE, fill = Highlight)) +
  geom_bar(stat = "identity", width = 0.6) +  # Bar chart
  geom_text(aes(label = round(MAE, 3)), vjust = -0.5, size = 3) +  # Add MAE values as text
  scale_fill_manual(values = c("Lowest MAE" = "green", "Other Models" = "grey")) +  # Custom colors
  labs(
    title = "Model Performance Comparison (Highlighting Lowest MAE)",
    x = "Model",
    y = "Mean Absolute Error (MAE)"
  ) +
  theme_minimal() +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    plot.title = element_text(size = 16, face = "bold"),
    legend.position = "none"
  )
```
## In our case, The MAE of lasso model is lowest, hence we choose lasso model for predicting the loss column for the given test dataset.

## I combined the train data that was split earlier for validation and retrained it with lasso regression model fo making predictions.

## Loading the results in the prediction file:
```{r}
# Step 1: Prepare the full training data (combine train_split and valid_split if applicable)
full_train <- rbind(train_split, valid_split)
X_full_train <- as.matrix(full_train[, -which(colnames(full_train) == "loss")])  # Predictor variables
y_full_train <- full_train$loss  # Target variable

# Step 2: Calculate scaling parameters for predictors
col_means <- colMeans(X_full_train)  # Column means of predictors
col_sds <- apply(X_full_train, 2, sd)  # Column standard deviations of predictors

# Step 3: Train the Lasso model on the full dataset
cv_lasso_final <- cv.glmnet(X_full_train, y_full_train, alpha = 1)  # Lasso regression
best_lambda_final <- cv_lasso_final$lambda.min  # Optimal lambda
cat("Best lambda for final Lasso model:", best_lambda_final, "\n")

# Step 4: Scale the test data using the training data's scaling parameters
X_test <- as.matrix(test_no_loss_lasso)  # Convert test predictors to matrix
X_test_scaled <- scale(X_test, center = col_means, scale = col_sds)  # Scale test predictors

# Step 5: Predict on the scaled test data
lasso_final_preds <- predict(cv_lasso_final, X_test_scaled, s = best_lambda_final)

# Step 6: Ensure predictions are on the original scale of 'loss' (no adjustment needed for glmnet)
lasso_final_preds <- as.numeric(lasso_final_preds)  # Convert predictions to numeric vector

# Clip predictions to the range of original loss values
lasso_final_preds <- pmax(pmin(lasso_final_preds, 100), 0)

# Round predictions to the nearest integer
lasso_final_preds <- round(lasso_final_preds)

# Step 7: Save predictions to a CSV file
submission <- data.frame(id = test_no_loss$id, loss = lasso_final_preds)
write.csv(submission, "final_predictions.csv", row.names = FALSE)
cat("Predictions saved to 'final_predictions.csv'\n")

# Step 8: Validate prediction range
cat("Range of original loss values:", range(y_full_train), "\n")
cat("Range of predicted loss values:", range(lasso_final_preds), "\n")



```
```{r}
# Read the predictions file
final_predictions <- read.csv("/Users/ritikakalyani/ADM Project/final_predictions.csv")
# Print the first few rows of the predictions
head(final_predictions,15)
```



